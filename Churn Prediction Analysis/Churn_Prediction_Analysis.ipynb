{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "NBgAOYPeyt-D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/Customer_Data.csv\")"
      ],
      "metadata": {
        "id": "7qbI-7jWywr1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the data to start fresh\n",
        "df = pd.read_csv(\"/content/Customer_Data.csv\")\n",
        "\n",
        "# Prepare the target variable - convert customer status to binary\n",
        "df['Churn'] = df['Customer_Status'].apply(lambda x: 1 if x == 'Churned' else 0)\n",
        "\n",
        "# Remove columns that would cause data leakage or aren't useful for prediction\n",
        "columns_to_drop = ['Customer_ID', 'Customer_Status', 'Churn_Category', 'Churn_Reason']\n",
        "df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "# Handle missing values by removing them\n",
        "df = df.dropna()\n",
        "\n",
        "# Convert categorical variables to numeric using one-hot encoding\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separate features from target\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "# Split data into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y  # Ensures balanced split\n",
        ")\n",
        "\n",
        "# Standardize features for better model performance\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the XGBoost model with optimized hyperparameters\n",
        "print(\"Training Enhanced XGBoost Classifier with optimized parameters...\")\n",
        "print(\"This may take a moment...\\n\")\n",
        "\n",
        "model = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        "\n",
        "    # Optimized hyperparameters for better performance\n",
        "    n_estimators=200,           # More trees for better learning\n",
        "    max_depth=6,                # Deeper trees to capture complex patterns\n",
        "    learning_rate=0.05,         # Slower learning for better generalization\n",
        "    subsample=0.8,              # Use 80% of data for each tree\n",
        "    colsample_bytree=0.8,       # Use 80% of features for each tree\n",
        "    min_child_weight=3,         # Minimum samples in leaf nodes\n",
        "    gamma=0.1,                  # Minimum loss reduction for split\n",
        "    scale_pos_weight=2,         # Handle class imbalance (more weight to churners)\n",
        "    reg_alpha=0.1,              # L1 regularization\n",
        "    reg_lambda=1.0              # L2 regularization\n",
        ")\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "print(\"‚úì Training completed!\\n\")\n",
        "\n",
        "# Generate predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Display model performance metrics\n",
        "print(\"=\"*60)\n",
        "print(\"MODEL PERFORMANCE EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nüìä Confusion Matrix:\")\n",
        "print(f\"    Predicted: No Churn | Predicted: Churn\")\n",
        "print(f\"    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
        "print(f\"Actually No Churn:  {conf_matrix[0][0]:>4}      |      {conf_matrix[0][1]:>4}\")\n",
        "print(f\"Actually Churn:     {conf_matrix[1][0]:>4}      |      {conf_matrix[1][1]:>4}\")\n",
        "\n",
        "print(\"\\nüìà Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
        "\n",
        "roc_score = roc_auc_score(y_test, y_proba)\n",
        "print(f\"üéØ ROC AUC Score: {roc_score:.4f}\")\n",
        "print(f\"   (Score ranges from 0.5 to 1.0, where 1.0 is perfect)\")\n",
        "\n",
        "# Calculate additional metrics for insights\n",
        "accuracy = (conf_matrix[0][0] + conf_matrix[1][1]) / conf_matrix.sum()\n",
        "sensitivity = conf_matrix[1][1] / (conf_matrix[1][0] + conf_matrix[1][1])  # Recall for churners\n",
        "specificity = conf_matrix[0][0] / (conf_matrix[0][0] + conf_matrix[0][1])  # Recall for non-churners\n",
        "\n",
        "print(f\"\\n‚úì Overall Accuracy: {accuracy:.2%}\")\n",
        "print(f\"‚úì Correctly Predicted: {conf_matrix[0][0] + conf_matrix[1][1]} out of {conf_matrix.sum()} cases\")\n",
        "print(f\"‚úì Churn Detection Rate (Sensitivity): {sensitivity:.2%}\")\n",
        "print(f\"‚úì Non-Churn Detection Rate (Specificity): {specificity:.2%}\")\n",
        "\n",
        "# Show top 10 most important features\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üîç TOP 10 MOST IMPORTANT FEATURES FOR CHURN PREDICTION\")\n",
        "print(\"=\"*60)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "for idx, row in feature_importance.head(10).iterrows():\n",
        "    bar_length = int(row['Importance'] * 50)\n",
        "    bar = '‚ñà' * bar_length\n",
        "    print(f\"{row['Feature'][:35]:35} {bar} {row['Importance']:.4f}\")\n",
        "\n",
        "# Generate predictions for the entire dataset\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Generating predictions for all customers...\")\n",
        "X_full_scaled = scaler.transform(X)\n",
        "df['Predicted_Churn'] = model.predict(X_full_scaled)\n",
        "df['Churn_Probability'] = model.predict_proba(X_full_scaled)[:, 1]\n",
        "\n",
        "# Risk segmentation\n",
        "df['Risk_Segment'] = pd.cut(\n",
        "    df['Churn_Probability'],\n",
        "    bins=[0, 0.3, 0.6, 1.0],\n",
        "    labels=['Low Risk', 'Medium Risk', 'High Risk']\n",
        ")\n",
        "\n",
        "# Save results\n",
        "output_file = \"Churn_Prediction_Output.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"‚úÖ Success! Predictions saved to '{output_file}'\")\n",
        "print(f\"   Total records: {len(df)}\")\n",
        "print(f\"   Predicted churners: {df['Predicted_Churn'].sum()} ({df['Predicted_Churn'].sum()/len(df):.1%})\")\n",
        "print(f\"\\nüìä Risk Segmentation:\")\n",
        "print(df['Risk_Segment'].value_counts().sort_index())\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9rUydZ20kbs",
        "outputId": "0b1f8fd7-220a-4134-b229-29d10520f5de"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Enhanced XGBoost Classifier with optimized parameters...\n",
            "This may take a moment...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:02] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Training completed!\n",
            "\n",
            "============================================================\n",
            "MODEL PERFORMANCE EVALUATION\n",
            "============================================================\n",
            "\n",
            "üìä Confusion Matrix:\n",
            "    Predicted: No Churn | Predicted: Churn\n",
            "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Actually No Churn:   233      |        40\n",
            "Actually Churn:       46      |        80\n",
            "\n",
            "üìà Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    No Churn       0.84      0.85      0.84       273\n",
            "       Churn       0.67      0.63      0.65       126\n",
            "\n",
            "    accuracy                           0.78       399\n",
            "   macro avg       0.75      0.74      0.75       399\n",
            "weighted avg       0.78      0.78      0.78       399\n",
            "\n",
            "üéØ ROC AUC Score: 0.8263\n",
            "   (Score ranges from 0.5 to 1.0, where 1.0 is perfect)\n",
            "\n",
            "‚úì Overall Accuracy: 78.45%\n",
            "‚úì Correctly Predicted: 313 out of 399 cases\n",
            "‚úì Churn Detection Rate (Sensitivity): 63.49%\n",
            "‚úì Non-Churn Detection Rate (Specificity): 85.35%\n",
            "\n",
            "============================================================\n",
            "üîç TOP 10 MOST IMPORTANT FEATURES FOR CHURN PREDICTION\n",
            "============================================================\n",
            "Value_Deal_Deal 5                   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.1841\n",
            "Contract_Two Year                   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.1248\n",
            "Contract_One Year                   ‚ñà‚ñà‚ñà 0.0719\n",
            "Internet_Type_Fiber Optic           ‚ñà‚ñà 0.0546\n",
            "Online_Security_Yes                 ‚ñà 0.0291\n",
            "Total_Revenue                       ‚ñà 0.0235\n",
            "Internet_Type_DSL                   ‚ñà 0.0207\n",
            "Value_Deal_Deal 3                    0.0199\n",
            "Payment_Method_Credit Card           0.0191\n",
            "Total_Charges                        0.0186\n",
            "\n",
            "============================================================\n",
            "Generating predictions for all customers...\n",
            "‚úÖ Success! Predictions saved to 'Churn_Prediction_Output.csv'\n",
            "   Total records: 1993\n",
            "   Predicted churners: 671 (33.7%)\n",
            "\n",
            "üìä Risk Segmentation:\n",
            "Risk_Segment\n",
            "Low Risk       1157\n",
            "Medium Risk     225\n",
            "High Risk       611\n",
            "Name: count, dtype: int64\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}